{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary contains the configuration necessary to divide each dataset used in the work, the script is made\n",
    "# for the conll type of datasets, so if you want to add datasets to divide, make sure to add it here\n",
    "gen_config = {\n",
    "    'leNER':{\n",
    "        'base_dir': \"../Base de Dados/leNER/\",\n",
    "        'load_dir': \"documents/allDoc/\",\n",
    "        'save_dir': \"divisions/iterative/\",\n",
    "        'col_sep' : ' ',\n",
    "        'file_type': '.conll',\n",
    "    },\n",
    "    'UlyssesNER-BR':{\n",
    "        'base_dir': \"../Base de Dados/UlyssesNER-BR/\",\n",
    "        'load_dir': \"PL_Corpus_byTypes_conll/\",\n",
    "        'save_dir': \"divisions/iterative/\",\n",
    "        'col_sep' : ' ',\n",
    "        'file_type': '.conll',\n",
    "    },\n",
    "    'Harem-second':{\n",
    "        'base_dir': \"../Base de Dados/HAREM/\",\n",
    "        'load_dir': \"SecondHarem/\",\n",
    "        'save_dir': \"divisions/second/iterative/\",\n",
    "        'col_sep' : '\\t',\n",
    "        'file_type': '.conll',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentencas(corpus, coll_sep):\n",
    "        sentenca = []\n",
    "        for line in corpus:\n",
    "            if line == '\\n':\n",
    "                if sentenca:\n",
    "                    yield sentenca\n",
    "                    sentenca = []\n",
    "            else:\n",
    "                \n",
    "                sentenca.append(line.strip('\\n').split(coll_sep))\n",
    "        if sentenca:\n",
    "            yield sentenca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filepath,coll_sep):\n",
    "    data_por_sentenca = []\n",
    "    with open(filepath, \"r\") as corpus:\n",
    "        data_por_sentenca += list(get_sentencas(corpus,coll_sep))\n",
    "    \n",
    "    return data_por_sentenca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config, selective = False, fileName = \"\"):\n",
    "    load_dir = config['base_dir'] + config['load_dir']\n",
    "    sep = config['col_sep']\n",
    "    if selective:\n",
    "        fp = load_dir + fileName\n",
    "        return read_dataset(filepath=fp, coll_sep=sep)\n",
    "    else:\n",
    "        filepaths = []\n",
    "        for file in os.listdir(load_dir):\n",
    "            if file.endswith(config['file_type']):\n",
    "                filepaths.append(load_dir + file)\n",
    "        \n",
    "        data_por_sentenca = []\n",
    "        for filepath in filepaths:\n",
    "            print(filepath+\"\\n\")\n",
    "            data_por_sentenca += read_dataset(filepath=filepath, coll_sep=sep)\n",
    "        \n",
    "        return data_por_sentenca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token_label(data):\n",
    "    return list(map(list,zip(*data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando DataSet e preparando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['secondHaremTotal', 'documents', 'divisions']\n",
      "Total de documentos carregados: 129\n"
     ]
    }
   ],
   "source": [
    "dataset = 'leNER'\n",
    "# dataset = 'Paramopama'\n",
    "# dataset = 'Harem-mini'\n",
    "# dataset = 'Harem-second'\n",
    "# dataset = 'UlyssesNER-BR'\n",
    "\n",
    "config = gen_config[dataset]\n",
    "\n",
    "filepaths = []\n",
    "\n",
    "files = os.listdir(config['base_dir'] + config['load_dir'])\n",
    "print(files)\n",
    "for file in files:\n",
    "    # if file.endswith('.conll'):\n",
    "    filepaths.append(config['base_dir'] + config['load_dir'] + file)\n",
    "\n",
    "# data_por_sentenca = get_data(config=config)\n",
    "\n",
    "# data_por_sentenca = get_data(config=config, selective=True, fileName='corpus_Paramopama.txt')\n",
    "# data_por_sentenca = get_data(config=config, selective=True, fileName='haremMini-total.colnn')\n",
    "# data_por_sentenca = get_data(config=config, selective=True, fileName='primeiroHAREM-total.colnn')\n",
    "data_por_sentenca_por_doc = {}\n",
    "for filepath, file in zip(filepaths, files):\n",
    "    data_por_sentenca_por_doc[file] = read_dataset(filepath, coll_sep=config['col_sep'])\n",
    "\n",
    "total_documentos = len(data_por_sentenca_por_doc)\n",
    "print(\"Total de documentos carregados:\", total_documentos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_por_sentenca_por_doc['HAREMdoc_aa64686'][0]\n",
    "# len(data_por_sentenca_por_doc['HAREMdoc_aa64686'])\n",
    "# data_por_sentenca_por_doc[list(data_por_sentenca_por_doc.keys())[0]]\n",
    "# d = list(data_por_sentenca_por_doc.values())[0][10][-1]\n",
    "# list(map(list,zip(d)))\n",
    "# # d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labels_por_doc = {}\n",
    "\n",
    "for docName, docData in data_por_sentenca_por_doc.items():\n",
    "    allDocData = []\n",
    "    \n",
    "    for sentence in docData:\n",
    "        allDocData += [i for i in sentence]\n",
    "    \n",
    "    docTokens, docLabels = split_token_label(allDocData)\n",
    "    \n",
    "    \n",
    "    labels_por_doc[docName] = docLabels\n",
    "\n",
    "    labels_count = Counter(docLabels)\n",
    "    \n",
    "    # Armazenar a contagem de rótulos para o documento atual\n",
    "    labels_por_doc[docName] = dict(labels_count)\n",
    "\n",
    "# Exibir as contagens de rótulos por documento\n",
    "for doc, labels_count in labels_por_doc.items():\n",
    "    print(f\"Documento: {doc}\")\n",
    "    print(\"Contagem de rótulos:\", labels_count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_por_doc['HAREMdoc_aa64686'][-12]\n",
    "# len(labels_por_doc['HAREMdoc_aa64686'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Inicializar um dicionário para armazenar contagens de rótulos por documento\n",
    "# antes e depois de simplificar.\n",
    "labels_por_doc = {}\n",
    "labels_por_doc_simplificado = {}\n",
    "\n",
    "for docName, docData in data_por_sentenca_por_doc.items():\n",
    "    allDocData = []\n",
    "    \n",
    "    # Concatenar todas as sentenças de um documento em uma lista\n",
    "    for sentence in docData:\n",
    "        allDocData += [i for i in sentence]\n",
    "    \n",
    "    # Separar tokens e rótulos\n",
    "    docTokens, docLabels = split_token_label(allDocData)\n",
    "    \n",
    "    # Contagem de rótulos com os prefixos (original)\n",
    "    labels_count_original = Counter(docLabels)\n",
    "    labels_por_doc[docName] = dict(labels_count_original)\n",
    "    \n",
    "    # Extração da última parte do rótulo (ex. 'B-PER' vira 'PER')\n",
    "    labels_simplificados = [lbl.split('-')[-1] for lbl in docLabels]\n",
    "    labels_count_simplificado = Counter(labels_simplificados)\n",
    "    labels_por_doc_simplificado[docName] = dict(labels_count_simplificado)\n",
    "\n",
    "# Exibir a contagem original e simplificada para comparação\n",
    "for doc in labels_por_doc.keys():\n",
    "    print(f\"Documento: {doc}\")\n",
    "    print(\"Contagem de rótulos (original):\", labels_por_doc[doc])\n",
    "    print(\"Contagem de rótulos (simplificado):\", labels_por_doc_simplificado[doc])\n",
    "    print()\n",
    "\n",
    "# Exibir o total de documentos\n",
    "print(\"Total de documentos:\", len(labels_por_doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_por_doc['HAREMdoc_aa64686'][-12]\n",
    "# len(labels_por_doc['HAREMdoc_aa64686'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_list = []\n",
    "for lbl in labels_por_doc.values():\n",
    "    lbl_list.extend(lbl)  # Adiciona os rótulos de cada documento à lista\n",
    "\n",
    "# Obtendo os rótulos únicos\n",
    "unique_lbl_list = list(set(lbl_list))\n",
    "\n",
    "# Exibindo a lista de rótulos únicos\n",
    "print(unique_lbl_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lbl_list.pop(unique_lbl_list.index('O'))\n",
    "unique_lbl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lbl_list = sorted(unique_lbl_list)\n",
    "unique_lbl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_l = ['LOCAL', 'ORGANIZACAO', 'PESSOA', 'TEMPO', 'LOCAL', 'ORGANIZACAO', 'PESSOA', 'TEMPO', 'LOCAL', 'ORGANIZACAO', 'PESSOA', 'TEMPO', 'TEMPO', 'TEMPO']\n",
    "#[test_l.count(x) for x in unique_lbl_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "initial_set = []\n",
    "for s_idx, s_labels in enumerate(list(labels_por_doc.values())):\n",
    "    # Contagem eficiente dos rótulos usando Counter\n",
    "    label_count = Counter(s_labels)\n",
    "    \n",
    "    # Adicionando o índice do documento e as contagens para cada rótulo em unique_lbl_list\n",
    "    initial_set.append([s_idx] + [label_count.get(x, 0) for x in unique_lbl_list])\n",
    "\n",
    "# Verificando o tamanho de initial_set (número de documentos)\n",
    "print(len(initial_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_set[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando a linha final com os totais de cada label (soma dos rótulos)\n",
    "initial_set.append(['-'] + [sum(x) for x in zip(*(row[1:] for row in initial_set))])\n",
    "\n",
    "# Agora vamos filtrar a matriz para manter apenas as 7 primeiras colunas\n",
    "# (assumindo que as 7 primeiras colunas correspondem aos rótulos -B)\n",
    "filtered_initial_set = []\n",
    "\n",
    "for row in initial_set:\n",
    "    # Mantendo apenas as 7 primeiras colunas (a primeira coluna é '-')\n",
    "    filtered_row = row[:7]  # Considerando que as primeiras 7 colunas são os rótulos -B\n",
    "    filtered_initial_set.append(filtered_row)\n",
    "\n",
    "# Exibindo a matriz resultante filtrada\n",
    "for row in filtered_initial_set:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(labels_por_doc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_set[-1] = initial_set[-1][:7]  # Isso mantém as 7 primeiras colunas, incluindo '-'\n",
    "\n",
    "# Exibindo a última linha após a atualização\n",
    "print(filtered_initial_set[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_initial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_divisions = 10\n",
    "result = [x / number_divisions for x in filtered_initial_set[-1][1:]]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_set = [[['-'] + [x/number_divisions for x in filtered_initial_set[-1][1:]]] for i in range(number_divisions)]\n",
    "sub_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "ITERATIVE STRATIFICATION\n",
    "\n",
    "- while there're still sentences in the initial_set\n",
    "    - update number of each label in the initial_set\n",
    "    - select label in initial_set that has fewest (at least 1) example\n",
    "    - then for each sentences that contains this label,\n",
    "        - find the subset that desires it more and add it to it\n",
    "        - update the desired number of each label for the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_set_l = [x for x in initial_set[:-1] if x[2] > 0]\n",
    "# len(initial_set_l)\n",
    "# 12168 - 2734\n",
    "# for sentence in initial_set_l:\n",
    "#     initial_set.pop(initial_set.index(sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(initial_set)\n",
    "# initial_set[-1]\n",
    "# initial_set[-1] = ['-'] + [ sum(x) for x in zip(*(row[1:] for row in initial_set[:-1])) ]\n",
    "# initial_set[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(filtered_initial_set) > 1:\n",
    "    print(f\"\\nTamanho atual de filtered_initial_set: {len(filtered_initial_set)}\")\n",
    "    contador = 0\n",
    "\n",
    "    try:\n",
    "        # Identificar o índice da label com menor quantidade\n",
    "        min_lbl_idx = filtered_initial_set[-1].index(min((i for i in filtered_initial_set[-1][1:] if i > 0)))\n",
    "        print(f\"Label com menor quantidade: {min_lbl_idx} (quantidade: {filtered_initial_set[-1][min_lbl_idx]})\")\n",
    "\n",
    "        # Filtrar as sentenças relevantes\n",
    "        initial_set_l = [x for x in filtered_initial_set[:-1] if x[min_lbl_idx] > 0]\n",
    "\n",
    "        # Ordenar as sentenças pela quantidade da palavra desejada (min_lbl_idx), em ordem decrescente\n",
    "        initial_set_l.sort(key=lambda x: x[min_lbl_idx], reverse=True)\n",
    "\n",
    "        print(f\"\\nSentenças ordenadas pela quantidade da palavra desejada (label {min_lbl_idx}):\")\n",
    "        for idx, item in enumerate(initial_set_l):\n",
    "            print(f\"  {idx + 1}: {item}\")\n",
    "\n",
    "        for sentence in initial_set_l:\n",
    "            # Verificar os tamanhos das partições\n",
    "            partition_sizes = [sum(sum(doc[1:]) for doc in partition[1:]) for partition in sub_set]\n",
    "            partitions_below_limit = [i for i, size in enumerate(partition_sizes) if size < 1225]\n",
    "\n",
    "            # Escolher a partição com menor valor total\n",
    "            if partitions_below_limit:\n",
    "                target_partition_idx = min(partitions_below_limit, key=lambda i: partition_sizes[i])\n",
    "            else:\n",
    "                target_partition_idx = partition_sizes.index(min(partition_sizes))\n",
    "\n",
    "            # Adicionar a sentença na partição escolhida\n",
    "            sub_set[target_partition_idx].append(sentence)\n",
    "            filtered_initial_set.pop(filtered_initial_set.index(sentence))\n",
    "\n",
    "            # Atualizar o desejo de cada label na partição\n",
    "            sub_set[target_partition_idx][0][1:] = [\n",
    "                i - j for i, j in zip(sub_set[target_partition_idx][0][1:], sentence[1:])\n",
    "            ]\n",
    "\n",
    "            # Atualizar os valores em filtered_initial_set\n",
    "            filtered_initial_set[-1] = ['-'] + [sum(x) for x in zip(*(row[1:] for row in filtered_initial_set[:-1]))]\n",
    "\n",
    "            # Imprimir status atualizado\n",
    "            total_entities_in_partition = partition_sizes[target_partition_idx] + sum(sentence[1:])\n",
    "            print(f\"\\nSentença adicionada: {sentence}\")\n",
    "            print(f\"Partição {target_partition_idx} agora tem {total_entities_in_partition} entidades.\")\n",
    "\n",
    "            contador += 1\n",
    "\n",
    "            # Recalcular a label com menor quantidade\n",
    "            min_lbl_idx = filtered_initial_set[-1].index(min((i for i in filtered_initial_set[-1][1:] if i > 0)))\n",
    "\n",
    "    except ValueError:\n",
    "        # Caso ValueError, processar as sentenças restantes sem basear-se na label\n",
    "        initial_set_l = filtered_initial_set[:-1]\n",
    "\n",
    "        print(f\"\\nConteúdo de initial_set_l (ValueError handler, {len(initial_set_l)} elementos):\")\n",
    "        for idx, item in enumerate(initial_set_l):\n",
    "            print(f\"  {idx + 1}: {item}\")\n",
    "\n",
    "        for sentence in initial_set_l:\n",
    "            # Verificar os tamanhos das partições\n",
    "            partition_sizes = [sum(sum(doc[1:]) for doc in partition[1:]) for partition in sub_set]\n",
    "            partitions_below_limit = [i for i, size in enumerate(partition_sizes) if size < 1300]\n",
    "\n",
    "            # Escolher a partição com menor valor total\n",
    "            if partitions_below_limit:\n",
    "                target_partition_idx = min(partitions_below_limit, key=lambda i: partition_sizes[i])\n",
    "            else:\n",
    "                target_partition_idx = partition_sizes.index(min(partition_sizes))\n",
    "\n",
    "            # Adicionar a sentença na partição escolhida\n",
    "            sub_set[target_partition_idx].append(sentence)\n",
    "            filtered_initial_set.pop(filtered_initial_set.index(sentence))\n",
    "\n",
    "            # Atualizar os valores em filtered_initial_set\n",
    "            filtered_initial_set[-1] = ['-'] + [sum(x) for x in zip(*(row[1:] for row in filtered_initial_set[:-1]))]\n",
    "\n",
    "            # Imprimir status atualizado\n",
    "            total_entities_in_partition = partition_sizes[target_partition_idx] + sum(sentence[1:])\n",
    "            print(f\"\\nSentença adicionada: {sentence}\")\n",
    "            print(f\"Partição {target_partition_idx} agora tem {total_entities_in_partition} entidades.\")\n",
    "\n",
    "    print(f\"\\nEstado atualizado de filtered_initial_set: {filtered_initial_set[-1]}\")\n",
    "    print(f\"Total de sentenças processadas nesta iteração: {contador}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_initial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sub_set:\n",
    "    print('tam: {} - lbs:{}'.format(len(sub[1:]), sub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sub_set:\n",
    "    \n",
    "    print (sub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sub_set[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando sobre cada subconjunto em sub_set\n",
    "for idx, s_set in enumerate(sub_set):\n",
    "    # Abrindo o arquivo para o subconjunto\n",
    "    file_path = f\"{config['base_dir']}{config['save_dir']}division_{idx}\"\n",
    "    with open(file_path, 'w+') as fp:\n",
    "        # Iterando sobre cada documento no subconjunto, excluindo a linha de contagens (assumindo que está na posição 0)\n",
    "        for set_doc in s_set[1:]:\n",
    "            # Obter o nome do documento usando o índice do documento\n",
    "            doc_name = list(data_por_sentenca_por_doc.keys())[set_doc[0]]\n",
    "            doc_sentences = data_por_sentenca_por_doc[doc_name]\n",
    "\n",
    "            # Escrever cada sentença do documento no arquivo\n",
    "            for sentence in doc_sentences:\n",
    "                for tk_class in sentence:\n",
    "                    # Gravar cada elemento da sentença, com separador adequado\n",
    "                    fp.write(config['col_sep'].join(map(str, tk_class)) + '\\n')\n",
    "                # Quebra de linha entre sentenças\n",
    "                fp.write('\\n')\n",
    "    \n",
    "    print(f\"Subconjunto {idx} salvo com sucesso em {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Diretório onde as partições estão salvas\n",
    "base_dir = \"../Base de Dados/leNER/divisions/iterative/\"\n",
    "\n",
    "# Inicializando o dicionário para armazenar contagens de entidades por partição\n",
    "qtd_classes_por_div = {}\n",
    "total_entidades_por_div = {}\n",
    "\n",
    "# Iterando sobre cada arquivo de partição\n",
    "for file_name in sorted(os.listdir(base_dir)):\n",
    "    if file_name.startswith(\"division_\"):  # Garantir que estamos pegando os arquivos corretos\n",
    "        partition_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "        # Inicializando o contador para esta partição\n",
    "        labels_count = Counter()\n",
    "\n",
    "        # Lendo o arquivo da partição\n",
    "        with open(partition_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:  # Se a linha não estiver vazia\n",
    "                    token_data = line.split()  # Separar os elementos da linha\n",
    "                    label = token_data[-1]  # O rótulo é o último elemento\n",
    "\n",
    "                    # Filtrar apenas os rótulos que começam com 'B-' e simplificar\n",
    "                    if label.startswith(\"B-\"):\n",
    "                        simplified_label = label.split(\"-\")[-1]  # Extrair a entidade (ex.: 'PER', 'LOC')\n",
    "                        labels_count[simplified_label] += 1\n",
    "\n",
    "        # Armazenar as contagens para a partição atual\n",
    "        qtd_classes_por_div[file_name] = labels_count\n",
    "        total_entidades_por_div[file_name] = sum(labels_count.values())\n",
    "\n",
    "# Calculando o total geral de todas as partições\n",
    "total_entidades_geral = sum(total_entidades_por_div.values())\n",
    "\n",
    "# Exibindo os totais por partição e o total geral\n",
    "print(\"Totais de entidades por partição:\")\n",
    "for div_name, total in total_entidades_por_div.items():\n",
    "    print(f\"  {div_name}: {total} entidades\")\n",
    "\n",
    "print(f\"\\nTotal geral de entidades em todas as partições: {total_entidades_geral} entidades\\n\")\n",
    "\n",
    "# Gerando os gráficos para cada partição\n",
    "for div_name, labels_count in qtd_classes_por_div.items():\n",
    "    # Ordenar os rótulos alfabeticamente\n",
    "    sorted_labels = sorted(labels_count.items())\n",
    "    classes = [label for label, _ in sorted_labels]\n",
    "    qtd = [count for _, count in sorted_labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(classes, qtd, color=\"pink\")\n",
    "    \n",
    "    # Adicionando os valores nas barras\n",
    "    for i, count in enumerate(qtd):\n",
    "        plt.text(count, i, str(count), va=\"center_baseline\", ha=\"right\", fontsize=10)\n",
    "\n",
    "    plt.title(f\"Entities per Class in Partition: {div_name}\", fontsize=15)\n",
    "    plt.xlabel(\"Number of Entities\", fontweight=\"bold\", fontsize=12)\n",
    "    plt.ylabel(\"Classes\", fontweight=\"bold\", fontsize=12)\n",
    "    \n",
    "    # Salvando o gráfico\n",
    "    output_path = os.path.join(base_dir, f\"entities_per_class_{div_name}.png\")\n",
    "    plt.savefig(output_path, dpi=300, format=\"png\", bbox_inches=\"tight\")\n",
    "    \n",
    "    print(f\"Gráfico salvo: {output_path}\")\n",
    "    # Exibir o gráfico (opcional)\n",
    "    # plt.show()\n",
    "\n",
    "print(\"Todos os gráficos foram gerados e salvos com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "074723ceb1146f2ece41e015a87b55f40b12aaf1d6f117cbb88935a39535fede"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
