{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_0.txt → ../Base de Dados/leNER/json_divisions/division_0.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_1.txt → ../Base de Dados/leNER/json_divisions/division_1.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_2.txt → ../Base de Dados/leNER/json_divisions/division_2.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_3.txt → ../Base de Dados/leNER/json_divisions/division_3.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_4.txt → ../Base de Dados/leNER/json_divisions/division_4.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_5.txt → ../Base de Dados/leNER/json_divisions/division_5.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_6.txt → ../Base de Dados/leNER/json_divisions/division_6.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_7.txt → ../Base de Dados/leNER/json_divisions/division_7.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_8.txt → ../Base de Dados/leNER/json_divisions/division_8.json\n",
      "Convertido: ../Base de Dados/leNER/divisions/iterative/division_9.txt → ../Base de Dados/leNER/json_divisions/division_9.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Entidades válidas\n",
    "valid_labels = {\n",
    "    'B-JURISPRUDENCIA', 'I-JURISPRUDENCIA',\n",
    "    'B-LEGISLACAO', 'I-LEGISLACAO',\n",
    "    'B-LOCAL', 'I-LOCAL',\n",
    "    'B-ORGANIZACAO', 'I-ORGANIZACAO',\n",
    "    'B-PESSOA', 'I-PESSOA',\n",
    "    'B-TEMPO', 'I-TEMPO'\n",
    "}\n",
    "\n",
    "# Pasta base onde estão as partições\n",
    "base_dir = \"../Base de Dados/leNER/divisions/iterative/\"\n",
    "output_dir = \"../Base de Dados/leNER/json_divisions/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Função para converter BIO para JSON estruturado\n",
    "def convert_bio_file_to_json(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    samples = []\n",
    "    tokens, labels = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tokens:\n",
    "                samples.append((tokens, labels))\n",
    "                tokens, labels = [], []\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) == 2:\n",
    "            token, label = parts\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "    \n",
    "    # última frase\n",
    "    if tokens:\n",
    "        samples.append((tokens, labels))\n",
    "\n",
    "    data_json = []\n",
    "    for tokens, labels in samples:\n",
    "        text = \" \".join(tokens)\n",
    "        entities = []\n",
    "        i = 0\n",
    "        while i < len(labels):\n",
    "            if labels[i].startswith(\"B-\"):\n",
    "                label_type = labels[i][2:]\n",
    "                entity_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                while i < len(labels) and labels[i].startswith(\"I-\") and labels[i][2:] == label_type:\n",
    "                    entity_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "                entities.append({\n",
    "                    \"text\": \" \".join(entity_tokens),\n",
    "                    \"label\": label_type\n",
    "                })\n",
    "            else:\n",
    "                i += 1\n",
    "        data_json.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": entities\n",
    "        })\n",
    "    \n",
    "    return data_json\n",
    "\n",
    "# Iterar sobre os 10 arquivos de divisão e salvar em JSON\n",
    "for i in range(10):\n",
    "    txt_file = os.path.join(base_dir, f\"division_{i}.txt\")\n",
    "    json_file = os.path.join(output_dir, f\"division_{i}.json\")\n",
    "    json_data = convert_bio_file_to_json(txt_file)\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Convertido: {txt_file} → {json_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 579 tensors from /shared/ModelsGGUF/hugging-quants/DeepSeek-R1-Distill-Qwen-14B-GGUF/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 14B\n",
      "llama_model_loader: - kv   5:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  25:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  26:                      quantize.imatrix.file str              = /models_out/DeepSeek-R1-Distill-Qwen-...\n",
      "llama_model_loader: - kv  27:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  28:             quantize.imatrix.entries_count i32              = 336\n",
      "llama_model_loader: - kv  29:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type q4_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 8.37 GiB (4.87 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151645 '<｜Assistant｜>' is not marked as EOG\n",
      "load: control token: 151644 '<｜User｜>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151647 '<|EOT|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "GPU disponível: NVIDIA GeForce RTX 3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 48\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 13824\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 14B\n",
      "print_info: model params     = 14.77 B\n",
      "print_info: general.name     = DeepSeek R1 Distill Qwen 14B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
      "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: layer  37 assigned to device CPU\n",
      "load_tensors: layer  38 assigned to device CPU\n",
      "load_tensors: layer  39 assigned to device CPU\n",
      "load_tensors: layer  40 assigned to device CPU\n",
      "load_tensors: layer  41 assigned to device CPU\n",
      "load_tensors: layer  42 assigned to device CPU\n",
      "load_tensors: layer  43 assigned to device CPU\n",
      "load_tensors: layer  44 assigned to device CPU\n",
      "load_tensors: layer  45 assigned to device CPU\n",
      "load_tensors: layer  46 assigned to device CPU\n",
      "load_tensors: layer  47 assigned to device CPU\n",
      "load_tensors: layer  48 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 578 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  8566.04 MiB\n",
      "...........................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   384.00 MiB\n",
      "llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   307.00 MiB\n",
      "llama_init_from_model: graph nodes  = 1686\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '336', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models_out/DeepSeek-R1-Distill-Qwen-14B-GGUF/DeepSeek-R1-Distill-Qwen-14B.imatrix', 'general.file_type': '15', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.bos_token_id': '151646', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'DeepSeek-R1-Distill-Qwen', 'qwen2.embedding_length': '5120', 'tokenizer.ggml.pre': 'deepseek-r1-qwen', 'general.name': 'DeepSeek R1 Distill Qwen 14B', 'qwen2.block_count': '48', 'general.type': 'model', 'general.size_label': '14B', 'qwen2.context_length': '131072', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\", 'qwen2.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '13824', 'qwen2.attention.layer_norm_rms_epsilon': '0.000010', 'qwen2.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\n",
      "Using chat eos_token: <｜end▁of▁sentence｜>\n",
      "Using chat bos_token: <｜begin▁of▁sentence｜>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "Exemplos few-shot usados: 6\n",
      "Exemplos few-shot usados para este fold:\n",
      "Texto: Rio Branco , 27 de março de 2018 .\n",
      "Entidades: [{\"text\": \"Rio Branco\", \"label\": \"LOCAL\"}, {\"text\": \"27 de março de 2018\", \"label\": \"TEMPO\"}]\n",
      "--------------------------------------------------\n",
      "Texto: Regina Ferrari Apelante : Acrediesel Comércio de Veículos Ltda Advogado : Thales Rocha Bordignon ( OAB : 2160/AC ) Advogada : Vanessa Fantin Mazoca de Almeida Prado ( OAB : 3956/AC ) Advogado : Mayara Cristine Bandeira de Lima ( OAB : 3580/AC ) Advogado : Gilliard Nobre Rocha ( OAB : 2833/AC ) Apelado : Auto Viação Floresta Cidade de Rio Branco Ltda Advogada : Márcia de Souza Nepomuceno ( OAB : 4181/RO ) Advogado : KLEIR SILVA CARVALHO ( OAB : 3432/AC ) Assunto : Direito Civil APELAÇÃO CÍVEL .\n",
      "Entidades: [{\"text\": \"Regina Ferrari\", \"label\": \"PESSOA\"}, {\"text\": \"Acrediesel Comércio de Veículos Ltda\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Thales Rocha Bordignon\", \"label\": \"PESSOA\"}, {\"text\": \"OAB\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Vanessa Fantin Mazoca de Almeida Prado\", \"label\": \"PESSOA\"}, {\"text\": \"OAB\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Mayara Cristine Bandeira de Lima\", \"label\": \"PESSOA\"}, {\"text\": \"OAB\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Gilliard Nobre Rocha\", \"label\": \"PESSOA\"}, {\"text\": \"OAB\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Auto Viação Floresta Cidade de Rio Branco Ltda\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Márcia de Souza Nepomuceno\", \"label\": \"PESSOA\"}, {\"text\": \"OAB\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"KLEIR SILVA CARVALHO\", \"label\": \"PESSOA\"}, {\"text\": \"OAB\", \"label\": \"ORGANIZACAO\"}]\n",
      "--------------------------------------------------\n",
      "Texto: PODER JUDICIÁRIO DO ESTADO DO ACRE Segunda Câmara Cível 1 Endereço : Rua Tribunal de Justiça , s/n , Via Verde , CEP 69.915-631 , Tel. 68 3302-0444/0445 , Rio BrancoAC - Mod . 500244 - Autos n.º 0715337-93.2014.8.01.0001 Acórdão n. : 5.582 Classe : Apelação n . 0715337-93.2014.8.01.0001 Origem : Rio Branco Órgão : Segunda Câmara Cível Relatora : Desª .\n",
      "Entidades: [{\"text\": \"ESTADO DO ACRE\", \"label\": \"LOCAL\"}, {\"text\": \"Segunda Câmara Cível\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Rua Tribunal de Justiça\", \"label\": \"LOCAL\"}, {\"text\": \"Via Verde\", \"label\": \"LOCAL\"}, {\"text\": \"Rio BrancoAC\", \"label\": \"LOCAL\"}, {\"text\": \"Autos n.º 0715337-93.2014.8.01.0001\", \"label\": \"JURISPRUDENCIA\"}, {\"text\": \"Acórdão n. : 5.582\", \"label\": \"JURISPRUDENCIA\"}, {\"text\": \"Apelação n . 0715337-93.2014.8.01.0001\", \"label\": \"JURISPRUDENCIA\"}, {\"text\": \"Rio Branco\", \"label\": \"LOCAL\"}, {\"text\": \"Segunda Câmara Cível\", \"label\": \"ORGANIZACAO\"}]\n",
      "--------------------------------------------------\n",
      "Texto: A técnica está prevista expressamente no art . 322 , § 1º , do CPC , o qual concretiza a nova perspectiva atribuída ao Processo Civil , voltada para uma efetiva prestação jurisdicional , para a justa composição da lide e para a proteção da boa- PODER JUDICIÁRIO DO ESTADO DO ACRE Segunda Câmara Cível 2 Endereço : Rua Tribunal de Justiça , s/n , Via Verde , CEP 69.915-631 , Tel. 68 3302-0444/0445 , Rio BrancoAC - Mod . 500244 - Autos n.º 0715337-93.2014.8.01.0001 fé . 2 .\n",
      "Entidades: [{\"text\": \"art . 322 , § 1º , do CPC\", \"label\": \"LEGISLACAO\"}, {\"text\": \"ESTADO DO ACRE\", \"label\": \"LOCAL\"}, {\"text\": \"Segunda Câmara Cível\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Rua Tribunal de Justiça\", \"label\": \"LOCAL\"}, {\"text\": \"Via Verde\", \"label\": \"LOCAL\"}, {\"text\": \"Rio BrancoAC\", \"label\": \"LOCAL\"}, {\"text\": \"Autos n.º 0715337-93.2014.8.01.0001\", \"label\": \"JURISPRUDENCIA\"}]\n",
      "--------------------------------------------------\n",
      "Texto: PODER JUDICIÁRIO DO ESTADO DO ACRE Segunda Câmara Cível 3 Endereço : Rua Tribunal de Justiça , s/n , Via Verde , CEP 69.915-631 , Tel. 68 3302-0444/0445 , Rio BrancoAC - Mod . 500244 - Autos n.º 0715337-93.2014.8.01.0001 Vistos , relatados e discutidos estes autos de Apelação n . 0715337-93.2014.8.01.0001 , DECIDE a Segunda Câmara Cível do Tribunal de Justiça do Estado do Acre , à unanimidade , DAR parcial provimento ao apelo , nos termos do voto da Desembargadora Relatora e das mídias digitais gravadas .\n",
      "Entidades: [{\"text\": \"ESTADO DO ACRE\", \"label\": \"LOCAL\"}, {\"text\": \"Segunda Câmara Cível\", \"label\": \"ORGANIZACAO\"}, {\"text\": \"Rua Tribunal de Justiça\", \"label\": \"LOCAL\"}, {\"text\": \"Via Verde\", \"label\": \"LOCAL\"}, {\"text\": \"Rio BrancoAC\", \"label\": \"LOCAL\"}, {\"text\": \"Autos n.º 0715337-93.2014.8.01.0001\", \"label\": \"JURISPRUDENCIA\"}, {\"text\": \"Apelação n . 0715337-93.2014.8.01.0001\", \"label\": \"JURISPRUDENCIA\"}, {\"text\": \"Segunda Câmara Cível do Tribunal de Justiça do Estado do Acre\", \"label\": \"ORGANIZACAO\"}]\n",
      "--------------------------------------------------\n",
      "Texto: Em razão do princípio da estabilidade da demanda , não se admite a alteração da causa de pedir , ainda que com o consentimento do réu , após o saneamento do processo , consoante vedação contida no art . 329 , II , do CPC .\n",
      "Entidades: [{\"text\": \"art . 329 , II , do CPC\", \"label\": \"LEGISLACAO\"}]\n",
      "--------------------------------------------------\n",
      "Total de exemplos no fold de teste: 775\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  164250.05 ms0/775 [00:00<?, ?it/s]\n",
      "llama_perf_context_print: prompt eval time =  164249.62 ms /  1484 tokens (  110.68 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32774.31 ms /   106 runs   (  309.19 ms per token,     3.23 tokens per second)\n",
      "llama_perf_context_print:       total time =  197126.50 ms /  1590 tokens\n",
      "Llama.generate: 1418 prefix-match hit, remaining 33 prompt tokens to eval13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução para exemplo: 197.1310 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  164250.05 ms\n",
      "llama_perf_context_print: prompt eval time =   11809.86 ms /    33 tokens (  357.87 ms per token,     2.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20666.71 ms /    69 runs   (  299.52 ms per token,     3.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   32540.47 ms /   102 tokens\n",
      "Llama.generate: 1419 prefix-match hit, remaining 23 prompt tokens to eval32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução para exemplo: 32.5446 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  164250.05 ms\n",
      "llama_perf_context_print: prompt eval time =   12231.41 ms /    23 tokens (  531.80 ms per token,     1.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11116.69 ms /    37 runs   (  300.45 ms per token,     3.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   23381.95 ms /    60 tokens\n",
      "Llama.generate: 1418 prefix-match hit, remaining 13 prompt tokens to eval19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução para exemplo: 23.3862 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  164250.05 ms\n",
      "llama_perf_context_print: prompt eval time =    8635.70 ms /    13 tokens (  664.28 ms per token,     1.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8330.54 ms /    28 runs   (  297.52 ms per token,     3.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   16992.01 ms /    41 tokens\n",
      "Llama.generate: 1418 prefix-match hit, remaining 17 prompt tokens to eval16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução para exemplo: 16.9962 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  164250.05 ms\n",
      "llama_perf_context_print: prompt eval time =    9592.18 ms /    17 tokens (  564.25 ms per token,     1.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8959.22 ms /    30 runs   (  298.64 ms per token,     3.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   18580.78 ms /    47 tokens\n",
      "Llama.generate: 1418 prefix-match hit, remaining 14 prompt tokens to eval22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução para exemplo: 18.5849 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  164250.05 ms\n",
      "llama_perf_context_print: prompt eval time =   15253.31 ms /    14 tokens ( 1089.52 ms per token,     0.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4731.65 ms /    16 runs   (  295.73 ms per token,     3.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   20000.09 ms /    30 tokens\n",
      "Llama.generate: 1418 prefix-match hit, remaining 23 prompt tokens to eval71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução para exemplo: 20.0044 segundos\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time\n",
    "import torch  # Para verificar GPU\n",
    "\n",
    "# Verifica se a GPU está disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU disponível: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"Nenhuma GPU disponível, utilizando CPU.\")\n",
    "\n",
    "llama_model_path = \"/shared/ModelsGGUF/hugging-quants/DeepSeek-R1-Distill-Qwen-14B-GGUF/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf\"\n",
    "\n",
    "# Inicializa o modelo Llama com GPU\n",
    "model = Llama(\n",
    "    model_path=llama_model_path,\n",
    "    n_ctx=2048,\n",
    "    n_gpu_layers=30\n",
    ")\n",
    "\n",
    "base_dir = \"../Base de Dados/leNER/json_divisions/\"\n",
    "division_files = [f\"division_{i}.json\" for i in range(10)]\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_all_labels():\n",
    "    return [\"JURISPRUDENCIA\", \"LEGISLACAO\", \"LOCAL\", \"ORGANIZACAO\", \"PESSOA\", \"TEMPO\"]\n",
    "\n",
    "def select_minimal_few_shot(dataset):\n",
    "    all_labels = set(get_all_labels())\n",
    "    selected = []\n",
    "    used_ids = set()\n",
    "    label_to_items = {label: [] for label in all_labels}\n",
    "    for item in dataset:\n",
    "        for ent in item['entities']:\n",
    "            label_to_items[ent['label']].append(item)\n",
    "    for label in all_labels:\n",
    "        for item in label_to_items[label]:\n",
    "            if id(item) not in used_ids:\n",
    "                selected.append(item)\n",
    "                used_ids.add(id(item))\n",
    "                break\n",
    "    return selected\n",
    "\n",
    "def truncate_text(text, max_tokens=150):\n",
    "    return text[:max_tokens] + \"...\" if len(text) > max_tokens else text\n",
    "\n",
    "def build_prompt(few_shot_examples, test_text):\n",
    "    prompt = \"\"\"\\\n",
    "Tarefa: Extraia entidades nomeadas do texto abaixo e retorne apenas uma lista JSON válida.\n",
    "Cada entidade deve ter os campos: \"text\" e \"label\".\n",
    "\n",
    "As entidades possíveis são:\n",
    "- JURISPRUDENCIA\n",
    "- LEGISLACAO\n",
    "- LOCAL\n",
    "- ORGANIZACAO\n",
    "- PESSOA\n",
    "- TEMPO\n",
    "\n",
    "Formato esperado:\n",
    "[\n",
    "  { \"text\": \"Supremo Tribunal\", \"label\": \"ORGANIZACAO\"},\n",
    "  { \"text\": \"Brasília\", \"label\": \"LOCAL\"}\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Texto: {truncate_text(ex['text'])}\\nEntidades: {json.dumps(ex['entities'], ensure_ascii=False)}\\n\\n\"\n",
    "    prompt += f\"Texto: {truncate_text(test_text)}\\nEntidades:\"\n",
    "    return prompt\n",
    "\n",
    "# Limpa o arquivo de relatório anterior\n",
    "open(\"relatorio_folds.txt\", \"w\").close()\n",
    "\n",
    "all_reports = []\n",
    "labels = get_all_labels()\n",
    "mlb = MultiLabelBinarizer(classes=labels)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nFold {i}\")\n",
    "    test_file = division_files[i]\n",
    "    train_files = [f for j, f in enumerate(division_files) if j != i]\n",
    "\n",
    "    test_data = load_data(os.path.join(base_dir, test_file))\n",
    "    train_data = []\n",
    "    for tf in train_files:\n",
    "        train_data.extend(load_data(os.path.join(base_dir, tf)))\n",
    "\n",
    "    few_shot = select_minimal_few_shot(train_data)\n",
    "    print(f\"Exemplos few-shot usados: {len(few_shot)}\")\n",
    "\n",
    "    print(\"Exemplos few-shot usados para este fold:\")\n",
    "    for ex in few_shot:\n",
    "        print(f\"Texto: {ex['text']}\")\n",
    "        print(f\"Entidades: {json.dumps(ex['entities'], ensure_ascii=False)}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(f\"Total de exemplos no fold de teste: {len(test_data)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    y_true_list, y_pred_list = [], []\n",
    "\n",
    "    for item in tqdm(test_data):\n",
    "        start_time = time.time()\n",
    "        prompt = build_prompt(few_shot, item['text'])\n",
    "\n",
    "        response = model(prompt, max_tokens=512, stop=[\"Texto:\"])\n",
    "        exec_time = time.time() - start_time\n",
    "        print(f\"Tempo de execução para exemplo: {exec_time:.4f} segundos\")\n",
    "\n",
    "        try:\n",
    "            output_text = response[\"choices\"][0][\"text\"].strip()\n",
    "            if not output_text.startswith(\"[\"):\n",
    "                output_text = output_text[output_text.find(\"[\"):]\n",
    "            pred_entities = json.loads(output_text)\n",
    "            if not isinstance(pred_entities, list):\n",
    "                pred_entities = []\n",
    "        except Exception:\n",
    "            pred_entities = []\n",
    "\n",
    "        y_true_labels = [ent['label'] for ent in item['entities'] if ent['label'] in labels]\n",
    "        y_pred_labels = [ent['label'] for ent in pred_entities if isinstance(ent, dict) and 'label' in ent and ent['label'] in labels]\n",
    "\n",
    "        y_true_list.append(y_true_labels)\n",
    "        y_pred_list.append(y_pred_labels)\n",
    "\n",
    "    y_true_bin = mlb.fit_transform(y_true_list)\n",
    "    y_pred_bin = mlb.transform(y_pred_list)\n",
    "\n",
    "    report = classification_report(y_true_bin, y_pred_bin, target_names=labels, output_dict=True, zero_division=0)\n",
    "    all_reports.append(report)\n",
    "\n",
    "    # ===== Salva relatório do fold no arquivo =====\n",
    "    report_lines = []\n",
    "\n",
    "    report_lines.append(f\"\\n====== FOLD {i} ======\")\n",
    "    report_lines.append(\"Relatório por label:\")\n",
    "\n",
    "    for label in labels:\n",
    "        report_lines.append(f\"- {label}: \"\n",
    "                            f\"precision={report[label]['precision']:.3f}, \"\n",
    "                            f\"recall={report[label]['recall']:.3f}, \"\n",
    "                            f\"f1-score={report[label]['f1-score']:.3f}\")\n",
    "\n",
    "    report_lines.append(f\"\\nMacro Avg: \"\n",
    "                        f\"precision={report['macro avg']['precision']:.3f}, \"\n",
    "                        f\"recall={report['macro avg']['recall']:.3f}, \"\n",
    "                        f\"f1-score={report['macro avg']['f1-score']:.3f}\")\n",
    "    report_lines.append(f\"Micro Avg: \"\n",
    "                        f\"precision={report['micro avg']['precision']:.3f}, \"\n",
    "                        f\"recall={report['micro avg']['recall']:.3f}, \"\n",
    "                        f\"f1-score={report['micro avg']['f1-score']:.3f}\")\n",
    "    report_lines.append(\"=\" * 50)\n",
    "\n",
    "    with open(\"relatorio_folds.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(report_lines) + \"\\n\")\n",
    "\n",
    "# ===== Relatório Final =====\n",
    "final_report = {}\n",
    "label_averages = {label: {\"precision\": 0, \"recall\": 0, \"f1\": 0} for label in labels}\n",
    "\n",
    "for label in labels:\n",
    "    precision = sum(r[label]['precision'] for r in all_reports if label in r) / len(all_reports)\n",
    "    recall = sum(r[label]['recall'] for r in all_reports if label in r) / len(all_reports)\n",
    "    f1 = sum(r[label]['f1-score'] for r in all_reports if label in r) / len(all_reports)\n",
    "    label_averages[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "macro_precision = sum(r[\"macro avg\"][\"precision\"] for r in all_reports) / len(all_reports)\n",
    "macro_recall = sum(r[\"macro avg\"][\"recall\"] for r in all_reports) / len(all_reports)\n",
    "macro_f1 = sum(r[\"macro avg\"][\"f1-score\"] for r in all_reports) / len(all_reports)\n",
    "\n",
    "micro_precision = sum(r[\"micro avg\"][\"precision\"] for r in all_reports) / len(all_reports)\n",
    "micro_recall = sum(r[\"micro avg\"][\"recall\"] for r in all_reports) / len(all_reports)\n",
    "micro_f1 = sum(r[\"micro avg\"][\"f1-score\"] for r in all_reports) / len(all_reports)\n",
    "\n",
    "# Salva resumo final no arquivo\n",
    "final_lines = []\n",
    "final_lines.append(\"\\n===== MÉDIA DOS 10 FOLDS =====\")\n",
    "for label in labels:\n",
    "    final_lines.append(f\"- {label}: \"\n",
    "                       f\"precision={label_averages[label]['precision']:.3f}, \"\n",
    "                       f\"recall={label_averages[label]['recall']:.3f}, \"\n",
    "                       f\"f1-score={label_averages[label]['f1']:.3f}\")\n",
    "\n",
    "final_lines.append(\"\\nMédias finais:\")\n",
    "final_lines.append(f\"Macro Avg: precision={macro_precision:.3f}, recall={macro_recall:.3f}, f1-score={macro_f1:.3f}\")\n",
    "final_lines.append(f\"Micro Avg: precision={micro_precision:.3f}, recall={micro_recall:.3f}, f1-score={micro_f1:.3f}\")\n",
    "\n",
    "with open(\"relatorio_folds.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(final_lines) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verifique se a GPU está disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Crie um tensor de exemplo\n",
    "input_tensor = torch.randn(1000, 1000)  # Um tensor de 1000x1000 com valores aleatórios\n",
    "\n",
    "# Mova o tensor para a GPU (se disponível)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Verifique se o tensor foi movido para a GPU\n",
    "print(input_tensor.device)  # Deve mostrar algo como 'cuda:0' se estiver usando a GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seu_modelo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Carregue o modelo\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mseu_modelo\u001b[49m()  \u001b[38;5;66;03m# Substitua por seu modelo real\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Mova o modelo para a GPU\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seu_modelo' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verifique se a GPU está disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carregue o modelo\n",
    "model = seu_modelo()  # Substitua por seu modelo real\n",
    "\n",
    "# Mova o modelo para a GPU\n",
    "model.to(device)\n",
    "\n",
    "# Se necessário, mova os dados (tensores) para a GPU também\n",
    "input_tensor = input_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ner_dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT pretrained model\n",
    "model_id = 'neuralmind/bert-base-portuguese-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Olá, isto é uma sentença!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ner_dataset[\"train\"][2]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example[f\"ner_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"ner_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = ner_dataset.map(tokenize_and_align_labels, batched=True, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][\"labels\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_id, num_labels=len(label_list), from_pt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_datasets[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "validation_set = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_set = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=compute_metrics, eval_dataset=validation_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# push_to_hub_model_id = f\"{model_name}-finetuned-{task}\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./tc_model_save/logs\")\n",
    "\n",
    "# push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=\"./tc_model_save\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     hub_model_id=push_to_hub_model_id,\n",
    "# )\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    \n",
    "    train_set,\n",
    "    validation_data=validation_set,\n",
    "    epochs=num_train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "db_path = \"optuna_study.db\"\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(\"Banco de dados deletado com sucesso.\")\n",
    "else:\n",
    "    print(\"O banco de dados não existe.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"optuna_study.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM trials\")\n",
    "print(cursor.fetchall())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./modelNER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, ner_labels):\n",
    "  all_predictions = []\n",
    "  all_labels = []\n",
    "  for batch in dataset:\n",
    "      logits = model.predict(batch)[\"logits\"]\n",
    "      labels = batch[\"labels\"]\n",
    "      predictions = np.argmax(logits, axis=-1)\n",
    "      for prediction, label in zip(predictions, labels):\n",
    "          for predicted_idx, label_idx in zip(prediction, label):\n",
    "              if label_idx == -100:\n",
    "                #   print(label)\n",
    "                  continue\n",
    "              all_predictions.append(ner_labels[predicted_idx])\n",
    "              all_labels.append(ner_labels[label_idx])\n",
    "              #print('\\npredicted=',ner_labels[predicted_idx], '\\nlabel=',ner_labels[label_idx])\n",
    "  #print(\"all_predictions=\",[all_predictions],'\\nall_labels=',[all_labels])\n",
    "  return metric.compute(predictions=[all_predictions], references=[all_labels])\n",
    "\n",
    "#results = evaluate(model, tf_eval_dataset, ner_labels=list(model.config.id2label.values()))\n",
    "results = evaluate(model, test_set, ner_labels=list(model.config.id2label.values()))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
