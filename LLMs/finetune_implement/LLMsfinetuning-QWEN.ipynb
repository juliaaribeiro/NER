{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Entidades válidas\n",
    "valid_labels = {\n",
    "    'B-LOCAL', 'I-LOCAL',\n",
    "    'B-ORGANIZACAO', 'I-ORGANIZACAO',\n",
    "    'B-PESSOA', 'I-PESSOA',\n",
    "    'B-TEMPO', 'I-TEMPO',\n",
    "}\n",
    "\n",
    "\n",
    "# Pasta base onde estão as partições\n",
    "base_dir = \"../Base de Dados/Paramopama/divisions/\"\n",
    "output_dir = \"../Base de Dados/Paramopama/divisions/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Função para converter BIO para JSON estruturado\n",
    "def convert_bio_file_to_json(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    samples = []\n",
    "    tokens, labels = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tokens:\n",
    "                samples.append((tokens, labels))\n",
    "                tokens, labels = [], []\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) == 2:\n",
    "            token, label = parts\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "    \n",
    "    # última frase\n",
    "    if tokens:\n",
    "        samples.append((tokens, labels))\n",
    "\n",
    "    data_json = []\n",
    "    for tokens, labels in samples:\n",
    "        text = \" \".join(tokens)\n",
    "        entities = []\n",
    "        i = 0\n",
    "        while i < len(labels):\n",
    "            if labels[i].startswith(\"B-\"):\n",
    "                label_type = labels[i][2:]\n",
    "                entity_tokens = [tokens[i]]\n",
    "                i += 1\n",
    "                while i < len(labels) and labels[i].startswith(\"I-\") and labels[i][2:] == label_type:\n",
    "                    entity_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "                entities.append({\n",
    "                    \"text\": \" \".join(entity_tokens),\n",
    "                    \"label\": label_type\n",
    "                })\n",
    "            else:\n",
    "                i += 1\n",
    "        data_json.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": entities\n",
    "        })\n",
    "    \n",
    "    return data_json\n",
    "\n",
    "# Iterar sobre os 10 arquivos de divisão e salvar em JSON\n",
    "for i in range(10):\n",
    "    txt_file = os.path.join(base_dir, f\"division_{i}.txt\")\n",
    "    json_file = os.path.join(output_dir, f\"division_{i}.json\")\n",
    "    json_data = convert_bio_file_to_json(txt_file)\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Convertido: {txt_file} → {json_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== FOLD 5 ===========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5983723908453ea5fe9d37a1b1a82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064c750aa38a4c8fb07ca24b40222b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69216f9ba66d4136b74f3202fe53304b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5256/3641492666.py:176: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/juliaribeiro/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='4818' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  11/4818 00:58 < 8:40:52, 0.15 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ========== CONFIGURAÇÕES ==========\n",
    "MODEL_NAME = \"/home/juliaribeiro/Qwen3-14B\"\n",
    "MAX_LEN = 2048\n",
    "NUM_EPOCHS = 3\n",
    "BASE_DIR = \"../Base de Dados/UlyssesNER-BR/json_divisions/\"\n",
    "LABELS = [\n",
    "    \"DATA\",\n",
    "    \"EVENTO\",\n",
    "    \"FUNDapelido\",\n",
    "    \"FUNDlei\",\n",
    "    \"FUNDprojetodelei\",\n",
    "    \"LOCALconcreto\",\n",
    "    \"LOCALvirtual\",\n",
    "    \"ORGgovernamental\",\n",
    "    \"ORGnaogovernamental\",\n",
    "    \"ORGpartido\",\n",
    "    \"PESSOAcargo\",\n",
    "    \"PESSOAgrupocargo\",\n",
    "    \"PESSOAindividual\",\n",
    "    \"PRODUTOoutros\",\n",
    "    \"PRODUTOprograma\",\n",
    "    \"PRODUTOsistema\"\n",
    "]\n",
    "\n",
    "\n",
    "os.makedirs(\"modelos_por_fold\", exist_ok=True)\n",
    "open(\"rrelatorio_folds.txt\", \"a\", encoding=\"utf-8\").close()\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=LABELS)\n",
    "\n",
    "# ========== FUNÇÕES AUXILIARES ==========\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def prepare_dataset(data):\n",
    "    samples = []\n",
    "    for item in data:\n",
    "        ents = item.get(\"entities\", [])\n",
    "        ents_fmt = [{\"text\": e[\"text\"], \"label\": e[\"label\"]} for e in ents if e[\"label\"] in LABELS]\n",
    "        if ents_fmt:\n",
    "            samples.append({\n",
    "                \"instruction\": \"Extraia entidades nomeadas do texto abaixo e retorne em JSON com campos 'text' e 'label'.\",\n",
    "                \"input\": item[\"text\"],\n",
    "                \"output\": json.dumps(ents_fmt, ensure_ascii=False),\n",
    "                \"labels\": [e[\"label\"] for e in ents_fmt]\n",
    "            })\n",
    "    return Dataset.from_list(samples)\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = f\"{example['instruction']}\\nTexto: {example['input']}\\nEntidades:\"\n",
    "    full_prompt = prompt + \" \" + example[\"output\"]\n",
    "\n",
    "    tokenized = tokenizer(full_prompt, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    tokenized[\"true_labels\"] = example[\"labels\"]\n",
    "\n",
    "    try:\n",
    "        entidades = json.loads(example[\"output\"])\n",
    "        if isinstance(entidades, list):\n",
    "            tokenized[\"true_entities\"] = entidades\n",
    "        else:\n",
    "            tokenized[\"true_entities\"] = []\n",
    "    except Exception:\n",
    "        tokenized[\"true_entities\"] = []\n",
    "\n",
    "    tokenized[\"input_text\"] = example[\"input\"]\n",
    "    return tokenized\n",
    "\n",
    "# ========== CHECKPOINT DE FOLD ==========\n",
    "\n",
    "CHECKPOINT_FILE = \"uultimo_fold.txt\"\n",
    "\n",
    "def read_last_fold():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            try:\n",
    "                return int(f.read().strip())\n",
    "            except:\n",
    "                return 0\n",
    "    return 0\n",
    "\n",
    "def write_last_fold(fold):\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        f.write(str(fold))\n",
    "\n",
    "# ========== MAIN: VALIDAÇÃO CRUZADA ==========\n",
    "\n",
    "def main():\n",
    "    division_files = [f\"division_{i}.json\" for i in range(10)]\n",
    "    all_reports = []\n",
    "    start_fold = read_last_fold()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=False, local_files_only=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    for fold in range(start_fold, 10):\n",
    "        if os.path.exists(f\"rresultados_fold_{fold}.json\"):\n",
    "            print(f\"Fold {fold} já processado. Pulando...\")\n",
    "            with open(f\"rresultados_fold_{fold}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                all_reports.append(json.load(f))\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=========== FOLD {fold} ===========\")\n",
    "\n",
    "        test_file = os.path.join(BASE_DIR, division_files[fold])\n",
    "        train_files = [os.path.join(BASE_DIR, f) for i, f in enumerate(division_files) if i != fold]\n",
    "\n",
    "        test_data = load_data(test_file)\n",
    "        train_data = []\n",
    "        for tf in train_files:\n",
    "            train_data.extend(load_data(tf))\n",
    "\n",
    "        ds_train = prepare_dataset(train_data)\n",
    "        ds_test = prepare_dataset(test_data)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            offload_buffers=True, \n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=64,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        tokenized_train = ds_train.map(lambda x: tokenize(x, tokenizer), batched=False)\n",
    "        tokenized_test = ds_test.map(lambda x: tokenize(x, tokenizer), batched=False)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"modelos_por_fold/fold_{fold}\",\n",
    "            per_device_train_batch_size=1,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=1e-4,\n",
    "            logging_steps=10,\n",
    "            bf16=True,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            push_to_hub=False\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        print(tokenized_test[0])\n",
    "\n",
    "        model.eval()\n",
    "        y_true_list, y_pred_list, predicted_entities_list = [], [], []\n",
    "\n",
    "        for item in tqdm(tokenized_test, desc=f\"Avaliando Fold {fold}\"):\n",
    "            input_text = f\"{item['instruction']}\\nTexto: {item['input']}\\nEntidades:\"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN).to(model.device)\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            try:\n",
    "                start = output_text.find(\"[\")\n",
    "                json_output = json.loads(output_text[start:]) if start != -1 else []\n",
    "                predicted = [e[\"label\"] for e in json_output if isinstance(e, dict) and e.get(\"label\") in LABELS]\n",
    "            except Exception:\n",
    "                json_output = []\n",
    "                predicted = []\n",
    "\n",
    "            y_true_list.append(item[\"true_labels\"])\n",
    "            y_pred_list.append(predicted)\n",
    "            predicted_entities_list.append(json_output)\n",
    "\n",
    "        \n",
    "        with open(f\"eerros_fold_{fold}.txt\", \"w\", encoding=\"utf-8\") as f_err:\n",
    "            for i, (true, pred) in enumerate(zip(y_true_list, y_pred_list)):\n",
    "                if set(true) != set(pred):\n",
    "                    item = tokenized_test[i]\n",
    "                    f_err.write(\"-\" * 80 + \"\\n\")\n",
    "                    f_err.write(f\"Texto:\\n{item['input']}\\n\\n\")\n",
    "                    f_err.write(\"Entidades verdadeiras:\\n\")\n",
    "                    f_err.write(json.dumps(item.get(\"true_entities\", []), ensure_ascii=False, indent=2))\n",
    "                    f_err.write(\"\\n\\nEntidades previstas:\\n\")\n",
    "                    f_err.write(json.dumps(predicted_entities_list[i], ensure_ascii=False, indent=2))\n",
    "                    f_err.write(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        y_true_bin = mlb.fit_transform(y_true_list)\n",
    "        y_pred_bin = mlb.transform(y_pred_list)\n",
    "\n",
    "        report = classification_report(y_true_bin, y_pred_bin, target_names=LABELS, output_dict=True, zero_division=0)\n",
    "        all_reports.append(report)\n",
    "\n",
    "        with open(f\"rresultados_fold_{fold}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        with open(\"rrelatorio_folds.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n====== FOLD {fold} ======\\n\")\n",
    "            for label in LABELS:\n",
    "                f.write(f\"- {label}: Precision={report[label]['precision']:.3f}, Recall={report[label]['recall']:.3f}, F1={report[label]['f1-score']:.3f}\\n\")\n",
    "            f.write(f\"\\nMacro Avg: Precision={report['macro avg']['precision']:.3f}, Recall={report['macro avg']['recall']:.3f}, F1={report['macro avg']['f1-score']:.3f}\\n\")\n",
    "            f.write(f\"Micro Avg: Precision={report['micro avg']['precision']:.3f}, Recall={report['micro avg']['recall']:.3f}, F1={report['micro avg']['f1-score']:.3f}\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        write_last_fold(fold + 1)\n",
    "\n",
    "        import gc\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    final_report = {}\n",
    "    for label in LABELS:\n",
    "        precision = sum(r[label]['precision'] for r in all_reports) / len(all_reports)\n",
    "        recall = sum(r[label]['recall'] for r in all_reports) / len(all_reports)\n",
    "        f1 = sum(r[label]['f1-score'] for r in all_reports) / len(all_reports)\n",
    "        final_report[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    macro_avg = {\n",
    "        \"precision\": sum(r[\"macro avg\"][\"precision\"] for r in all_reports) / len(all_reports),\n",
    "        \"recall\": sum(r[\"macro avg\"][\"recall\"] for r in all_reports) / len(all_reports),\n",
    "        \"f1\": sum(r[\"macro avg\"][\"f1-score\"] for r in all_reports) / len(all_reports),\n",
    "    }\n",
    "\n",
    "    micro_avg = {\n",
    "        \"precision\": sum(r[\"micro avg\"][\"precision\"] for r in all_reports) / len(all_reports),\n",
    "        \"recall\": sum(r[\"micro avg\"][\"recall\"] for r in all_reports) / len(all_reports),\n",
    "        \"f1\": sum(r[\"micro avg\"][\"f1-score\"] for r in all_reports) / len(all_reports),\n",
    "    }\n",
    "\n",
    "    with open(\"rrelatorio_final.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== MÉDIAS DOS 10 FOLDS ===\\n\\n\")\n",
    "        for label, scores in final_report.items():\n",
    "            f.write(f\"{label}: Precision={scores['precision']:.3f}, Recall={scores['recall']:.3f}, F1={scores['f1']:.3f}\\n\")\n",
    "        f.write(f\"\\nMacro Avg:\\nPrecision={macro_avg['precision']:.3f}, Recall={macro_avg['recall']:.3f}, F1={macro_avg['f1']:.3f}\\n\")\n",
    "        f.write(f\"\\nMicro Avg:\\nPrecision={micro_avg['precision']:.3f}, Recall={micro_avg['recall']:.3f}, F1={micro_avg['f1']:.3f}\\n\")\n",
    "\n",
    "    print(\"\\u2705 Processamento completo. Relatórios gerados.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:963\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# used.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 97)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Exemplo de uso:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m texto_exemplo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbert Einstein nasceu na Alemanha.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m entidades \u001b[38;5;241m=\u001b[39m \u001b[43mextrair_entidades_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto_exemplo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(entidades)\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mextrair_entidades_gemma\u001b[0;34m(texto, modelo)\u001b[0m\n\u001b[1;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m     25\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m---> 27\u001b[0m resposta_texto \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Tenta extrair o JSON das entidades do texto retornado\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Encontrar a primeira ocorrência de [ para tentar extrair a lista JSON\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 971\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 97)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def extrair_entidades_gemma(texto, modelo=\"gemma:2b\"):\n",
    "    prompt = (\n",
    "        \"Extraia as entidades nomeadas do texto abaixo e retorne apenas um JSON com a lista das entidades, \"\n",
    "        \"cada uma com o texto e o tipo.\\n\\n\"\n",
    "        \"Exemplo:\\n\"\n",
    "        \"Texto: John Lennon era músico.\\n\"\n",
    "        \"Entidades: [{\\\"texto\\\": \\\"John Lennon\\\", \\\"tipo\\\": \\\"Pessoa\\\"}]\\n\\n\"\n",
    "        \"Agora extraia as entidades do texto:\\n\"\n",
    "        f\"Texto: {texto}\\n\"\n",
    "        \"Entidades:\"\n",
    "    )\n",
    "\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": modelo,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    resposta_texto = response.json().get(\"response\", \"\")\n",
    "    \n",
    "    # Tenta extrair o JSON das entidades do texto retornado\n",
    "    try:\n",
    "        # Encontrar a primeira ocorrência de [ para tentar extrair a lista JSON\n",
    "        start = resposta_texto.find(\"[\")\n",
    "        end = resposta_texto.rfind(\"]\") + 1\n",
    "        json_str = resposta_texto[start:end]\n",
    "        entidades = json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao extrair JSON:\", e)\n",
    "        entidades = None\n",
    "\n",
    "    return entidades\n",
    "\n",
    "# Exemplo de uso:\n",
    "texto_exemplo = \"Albert Einstein nasceu na Alemanha.\"\n",
    "entidades = extrair_entidades_gemma(texto_exemplo)\n",
    "print(entidades)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "expected value at line 1 column 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/juliaribeiro/Qwen3-14B/Qwen3-14B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Tokenizer carregado com sucesso!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexemplo de texto\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1013\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m         )\n\u001b[0;32m-> 1013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2025\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2023\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2278\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2278\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2280\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2283\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/qwen2/tokenization_qwen2_fast.py:120\u001b[0m, in \u001b[0;36mQwen2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m unk_token \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    110\u001b[0m     AddedToken(unk_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(unk_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m unk_token\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m pad_token \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m     AddedToken(pad_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[1;32m    118\u001b[0m )\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerges_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:117\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: expected value at line 1 column 1"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/juliaribeiro/Qwen3-14B/Qwen3-14B\", trust_remote_code=False, local_files_only=True)\n",
    "print(\"✅ Tokenizer carregado com sucesso!\")\n",
    "print(tokenizer(\"exemplo de texto\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Olá, isto é uma sentença!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ner_dataset[\"train\"][2]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example[f\"ner_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"ner_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = ner_dataset.map(tokenize_and_align_labels, batched=True, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][\"labels\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_id, num_labels=len(label_list), from_pt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_datasets[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "validation_set = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_set = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=compute_metrics, eval_dataset=validation_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# push_to_hub_model_id = f\"{model_name}-finetuned-{task}\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./tc_model_save/logs\")\n",
    "\n",
    "# push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=\"./tc_model_save\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     hub_model_id=push_to_hub_model_id,\n",
    "# )\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    \n",
    "    train_set,\n",
    "    validation_data=validation_set,\n",
    "    epochs=num_train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "db_path = \"optuna_study.db\"\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(\"Banco de dados deletado com sucesso.\")\n",
    "else:\n",
    "    print(\"O banco de dados não existe.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"optuna_study.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM trials\")\n",
    "print(cursor.fetchall())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./modelNER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, ner_labels):\n",
    "  all_predictions = []\n",
    "  all_labels = []\n",
    "  for batch in dataset:\n",
    "      logits = model.predict(batch)[\"logits\"]\n",
    "      labels = batch[\"labels\"]\n",
    "      predictions = np.argmax(logits, axis=-1)\n",
    "      for prediction, label in zip(predictions, labels):\n",
    "          for predicted_idx, label_idx in zip(prediction, label):\n",
    "              if label_idx == -100:\n",
    "                #   print(label)\n",
    "                  continue\n",
    "              all_predictions.append(ner_labels[predicted_idx])\n",
    "              all_labels.append(ner_labels[label_idx])\n",
    "              #print('\\npredicted=',ner_labels[predicted_idx], '\\nlabel=',ner_labels[label_idx])\n",
    "  #print(\"all_predictions=\",[all_predictions],'\\nall_labels=',[all_labels])\n",
    "  return metric.compute(predictions=[all_predictions], references=[all_labels])\n",
    "\n",
    "#results = evaluate(model, tf_eval_dataset, ner_labels=list(model.config.id2label.values()))\n",
    "results = evaluate(model, test_set, ner_labels=list(model.config.id2label.values()))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unsloth)",
   "language": "python",
   "name": "unsloth-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
