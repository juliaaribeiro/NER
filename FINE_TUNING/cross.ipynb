{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, AutoConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Configuração do modelo e tarefa\n",
    "task = \"ner\"\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "batch_size = 8\n",
    "label_all_tokens = True\n",
    "\n",
    "# Carregar o tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Caminho para as partições\n",
    "base_path = \"../models/\"\n",
    "\n",
    "# Carregar as partições diretamente do caminho especificado\n",
    "tokenized_datasets = {}\n",
    "for i in range(10):\n",
    "    partition_path = f\"{base_path}dataset_division_{i}\"\n",
    "    if os.path.exists(partition_path):  # Verifica se o diretório existe\n",
    "        try:\n",
    "            tokenized_datasets[f\"particao_{i}\"] = load_from_disk(partition_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar a partição {partition_path}: {e}\")\n",
    "\n",
    "# Verificar as chaves disponíveis em uma amostra dos dados\n",
    "sample = tokenized_datasets[\"particao_0\"][:2]  # Pegando 2 amostras da partição\n",
    "print(\"Exemplo de dados:\", sample)\n",
    "print(\"Chaves disponíveis no dataset:\", sample.keys())\n",
    "\n",
    "# Acessar as labels corretamente para construir label2id\n",
    "label_list = tokenized_datasets[\"particao_0\"].features[\"sentences\"][\"labels\"].feature\n",
    "label_names = label_list.dtype\n",
    "\n",
    "# Criar o mapeamento de label2id\n",
    "label2id = {\n",
    "    'O': 0,               # Fora de entidade\n",
    "    'B-ORGANIZACAO': 1,    # Início da entidade ORGANIZACAO\n",
    "    'I-ORGANIZACAO': 2,    # Interior da entidade ORGANIZACAO\n",
    "    'B-JURISPRUDENCIA': 3, # Início da entidade JURISPRUDENCIA\n",
    "    'I-JURISPRUDENCIA': 4, # Interior da entidade JURISPRUDENCIA\n",
    "    'B-LOCAL': 5,          # Início da entidade LOCAL\n",
    "    'I-LOCAL': 6,          # Interior da entidade LOCAL\n",
    "    'B-LEGISLACAO': 7,     # Início da entidade LEGISLACAO\n",
    "    'I-LEGISLACAO': 8,     # Interior da entidade LEGISLACAO\n",
    "    'B-PESSOA': 9,         # Início da entidade PESSOA\n",
    "    'I-PESSOA': 10,        # Interior da entidade PESSOA\n",
    "    'B-TEMPO': 11,         # Início da entidade TEMPO\n",
    "    'I-TEMPO': 12          # Interior da entidade TEMPO\n",
    "}\n",
    "\n",
    "# Exibir o mapeamento de labels\n",
    "print(f\"Label2ID mapping: {label2id}\")\n",
    "\n",
    "# Função para calcular métricas\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Calcular métricas de precisão, recall, F1 e acurácia\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    # Depuração: Exibindo as métricas para cada classe\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # Calculando a média macro das métricas\n",
    "    macro_precision = np.mean([precision[label] for label in label_names])\n",
    "    macro_recall = np.mean([recall[label] for label in label_names])\n",
    "    macro_f1 = np.mean([f1[label] for label in label_names])\n",
    "\n",
    "    print(f\"Macro Precision: {macro_precision}\")\n",
    "    print(f\"Macro Recall: {macro_recall}\")\n",
    "    print(f\"Macro F1: {macro_f1}\")\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "# Função para tokenizar e alinhar as labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Verificando se 'sentences' está presente nos dados\n",
    "    if \"sentences\" not in examples:\n",
    "        raise KeyError(\"A chave 'sentences' não foi encontrada nos dados.\")\n",
    "\n",
    "    tokenized_inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for sentence in examples[\"sentences\"]:\n",
    "        # Tokenizando os tokens de cada sentença\n",
    "        tokenized_sentence = tokenizer(sentence[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "        word_ids = tokenized_sentence.word_ids()  # Pega os ids dos tokens\n",
    "        sentence_labels = sentence[\"labels\"]  # Labels da sentença\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        # Atribuindo as labels corretamente\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special token (e.g., [CLS], [SEP])\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Verificando se a label existe em label2id\n",
    "                label = sentence_labels[word_idx]\n",
    "                if label not in label2id:\n",
    "                    print(f\"Label {label} não encontrada em label2id!\")  # Print de depuração\n",
    "                label_ids.append(label2id.get(label, -1))  # Usa -1 se a label não for encontrada\n",
    "            else:\n",
    "                label_ids.append(label2id.get(sentence_labels[word_idx], -1) if label_all_tokens else -100)\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        tokenized_inputs.append(tokenized_sentence)\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Atribuindo as labels ao tokenized_inputs\n",
    "    for i, tokenized_sentence in enumerate(tokenized_inputs):\n",
    "        tokenized_sentence[\"labels\"] = labels[i]\n",
    "\n",
    "    # Retorna os dados tokenizados e alinhados com as labels\n",
    "    return {\"input_ids\": [x[\"input_ids\"] for x in tokenized_inputs],\n",
    "            \"attention_mask\": [x[\"attention_mask\"] for x in tokenized_inputs],\n",
    "            \"labels\": labels}\n",
    "\n",
    "# Configurações de treinamento\n",
    "if __name__ == \"__main__\":\n",
    "    # Tokenizando e alinhando as labels para todas as partições\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = {key: dataset.map(tokenize_and_align_labels, batched=True) for key, dataset in tokenized_datasets.items()}\n",
    "\n",
    "    hidden_dropout_prob = 0.3506306968358118\n",
    "    attention_probs_dropout_prob = 0.18770484420356393\n",
    "    learning_rate = 2.339919292660243e-05\n",
    "    num_train_epochs = 32\n",
    "\n",
    "    # Configuração do modelo\n",
    "    configuration_base = AutoConfig.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        num_labels=len(label2id), \n",
    "        label2id=label2id, \n",
    "        id2label={i: label for i, label in enumerate(label2id)}\n",
    "    )\n",
    "    configuration_base.hidden_dropout_prob = hidden_dropout_prob\n",
    "    configuration_base.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, config=configuration_base)\n",
    "\n",
    "    # Argumentos de treinamento\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    training_args = TrainingArguments(\n",
    "        f\"{model_name}-finetuned-{task}\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy='no',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "    )\n",
    "\n",
    "    # Treinamento usando todas as 10 partições\n",
    "    results = []\n",
    "    per_class_results = []\n",
    "    sets = [x for x in range(10)]  # Usando todas as 10 partições\n",
    "\n",
    "    for test_set in sets:\n",
    "        tokenized_datasets['test'] = tokenized_datasets[f'particao_{test_set}']\n",
    "        tokenized_datasets['train'] = concatenate_datasets([tokenized_datasets[f'particao_{x}'] for x in sets if x != test_set])\n",
    "        \n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, config=configuration_base)\n",
    "        \n",
    "        print(f\"Training model for test set {test_set}...\")\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],  # Passar o dataset de teste aqui\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        eval_result = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "        results.append(eval_result)\n",
    "        trainer.save_model(f'./bertimbal-Save-{test_set}/')\n",
    "\n",
    "        # Pre class results\n",
    "        predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        per_class_results.append(SeqEval().compute(predictions=true_predictions, references=true_labels))\n",
    "\n",
    "    print(f'OVERALL: {results}')\n",
    "    print(f'PER CLASS: {per_class_results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install \"transformers[torch]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfb738fe6204026b4632cd1e06c1bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_1 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54f153ffe73459a8de63badc1556c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_2 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5160f7b6ff50467fa854a2757c73c5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_3 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6970c4af1aa4ae39027469ea2aa62a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_4 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b0bd8a7f184a6db7b07a3ac3da81ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1363 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_5 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0e555363ad4590a02e16e43f53d158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_6 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba08929292b4401a24a88dd687320fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_7 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e751f481dc5489cadb0193c8f0c7494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_8 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e967911959b4788960a649e97253076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1415 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_9 criado com sucesso!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5ddde2487741a98e454c2f2b24ff57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset para particao_10 criado com sucesso!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ../models/dat_particao_0 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdat_particao_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Carregando o dataset da partição\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Exibindo as primeiras 5 entradas do dataset\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset[:\u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/load.py:2207\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2205\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2209\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2210\u001b[0m ):\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ../models/dat_particao_0 not found"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "# Função para processar um arquivo txt e criar o dataset\n",
    "def txt_to_dataset(file_path):\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append({\n",
    "                        'tokens': tokens,\n",
    "                        'labels': labels\n",
    "                    })\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:\n",
    "                token, label = parts\n",
    "            elif len(parts) > 2:\n",
    "                token = \" \".join(parts[:-1])\n",
    "                label = parts[-1]\n",
    "            else:\n",
    "                continue  # ignora linhas vazias ou inválidas\n",
    "\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "\n",
    "    if tokens:\n",
    "        sentences.append({\n",
    "            'tokens': tokens,\n",
    "            'labels': labels\n",
    "        })\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Caminho para a pasta onde os arquivos estão localizados\n",
    "base_path = \"../Base de Dados/cachacaNER/\"\n",
    "\n",
    "# Processando cada arquivo de division_0.txt a division_9.txt\n",
    "for i in range(1,11):\n",
    "    file_path = os.path.join(base_path, f\"particao_{i}.txt\")\n",
    "    sentences = txt_to_dataset(file_path)\n",
    "    \n",
    "    # Criando o dataset para o arquivo atual\n",
    "    dataset = Dataset.from_dict({\"sentences\": sentences})\n",
    "    \n",
    "    # Salvando o dataset com um nome correspondente ao arquivo\n",
    "    dataset.save_to_disk(f\"dat_division_{i}\")\n",
    "    \n",
    "    print(f\"Dataset para particao_{i} criado com sucesso!\")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Caminho para o diretório onde os datasets estão salvos\n",
    "save_dir = \"../models/\"  # ou qualquer outro caminho que você tenha usado\n",
    "\n",
    "# Número da partição/divisão que você deseja carregar (por exemplo, division_0)\n",
    "partition_number = 0  # Alterar para o número da partição que deseja carregar\n",
    "\n",
    "# Caminho para o dataset da partição específica\n",
    "dataset_path = os.path.join(save_dir, f\"dat_particao_{partition_number}\")\n",
    "\n",
    "# Carregando o dataset da partição\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Exibindo as primeiras 5 entradas do dataset\n",
    "print(dataset[:5])\n",
    "# Verificando as features de um dos datasets para encontrar a chave correta\n",
    "print (\"TERMINA\")\n",
    "print(tokenized_datasets[\"particao_0\"].features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[\"train\"].features[f\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ner_dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT pretrained model\n",
    "model_id = 'neuralmind/bert-base-portuguese-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Olá, isto é uma sentença!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ner_dataset[\"train\"][2]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example[f\"ner_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"ner_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = ner_dataset.map(tokenize_and_align_labels, batched=True, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][\"labels\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_id, num_labels=len(label_list), from_pt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_datasets[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "validation_set = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_set = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=compute_metrics, eval_dataset=validation_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# push_to_hub_model_id = f\"{model_name}-finetuned-{task}\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./tc_model_save/logs\")\n",
    "\n",
    "# push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=\"./tc_model_save\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     hub_model_id=push_to_hub_model_id,\n",
    "# )\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    \n",
    "    train_set,\n",
    "    validation_data=validation_set,\n",
    "    epochs=num_train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco de dados deletado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "db_path = \"optuna_study.db\"\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(\"Banco de dados deletado com sucesso.\")\n",
    "else:\n",
    "    print(\"O banco de dados não existe.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0, 1, 'COMPLETE', '2025-03-17 17:45:05.971460', '2025-03-17 22:18:47.019100'), (2, 1, 1, 'COMPLETE', '2025-03-17 22:18:47.035072', '2025-03-18 00:52:06.921875'), (3, 2, 1, 'COMPLETE', '2025-03-18 00:52:06.949967', '2025-03-18 01:02:00.231359'), (4, 3, 1, 'COMPLETE', '2025-03-18 01:02:00.246875', '2025-03-18 06:39:38.600319'), (5, 4, 1, 'COMPLETE', '2025-03-18 06:39:38.621693', '2025-03-18 06:50:25.657802'), (6, 5, 1, 'COMPLETE', '2025-03-18 06:50:25.670239', '2025-03-18 07:00:35.832535'), (7, 6, 1, 'COMPLETE', '2025-03-18 07:00:35.845027', '2025-03-18 09:48:10.888379'), (8, 7, 1, 'COMPLETE', '2025-03-18 09:48:10.909911', '2025-03-18 09:53:29.625023'), (9, 8, 1, 'COMPLETE', '2025-03-18 09:53:29.637677', '2025-03-18 12:39:59.181870'), (10, 9, 1, 'COMPLETE', '2025-03-18 12:39:59.194094', '2025-03-18 12:44:38.925507'), (11, 10, 1, 'COMPLETE', '2025-03-18 12:44:38.937566', '2025-03-18 12:48:09.540221'), (12, 11, 1, 'COMPLETE', '2025-03-18 12:48:09.552365', '2025-03-18 12:50:52.469633'), (13, 12, 1, 'COMPLETE', '2025-03-18 12:50:52.482195', '2025-03-18 13:00:06.225502'), (14, 13, 1, 'COMPLETE', '2025-03-18 13:00:06.237423', '2025-03-18 13:21:27.088313'), (15, 14, 1, 'COMPLETE', '2025-03-18 13:21:27.100348', '2025-03-18 13:23:53.707049'), (16, 15, 1, 'COMPLETE', '2025-03-18 13:23:53.718940', '2025-03-18 14:00:50.379104'), (17, 16, 1, 'COMPLETE', '2025-03-18 14:00:50.391302', '2025-03-18 14:03:34.690591'), (18, 17, 1, 'COMPLETE', '2025-03-18 14:03:34.702550', '2025-03-18 16:53:40.015591'), (19, 18, 1, 'COMPLETE', '2025-03-18 16:53:40.027987', '2025-03-18 16:57:10.558460'), (20, 19, 1, 'COMPLETE', '2025-03-18 16:57:10.570604', '2025-03-18 17:35:03.148048'), (21, 20, 1, 'COMPLETE', '2025-03-18 17:35:03.160178', '2025-03-18 17:45:45.354599'), (22, 21, 1, 'COMPLETE', '2025-03-18 17:45:45.366485', '2025-03-18 17:56:28.119396'), (23, 22, 1, 'COMPLETE', '2025-03-18 17:56:28.131574', '2025-03-18 18:07:12.749153'), (24, 23, 1, 'COMPLETE', '2025-03-18 18:07:12.761307', '2025-03-18 18:28:36.638135'), (25, 24, 1, 'COMPLETE', '2025-03-18 18:28:36.649941', '2025-03-18 18:39:18.259159'), (26, 0, 2, 'COMPLETE', '2025-03-18 18:51:20.228703', '2025-03-18 19:09:02.762142'), (27, 1, 2, 'COMPLETE', '2025-03-18 19:09:02.776140', '2025-03-18 21:36:35.831703'), (28, 2, 2, 'COMPLETE', '2025-03-18 21:36:35.844212', '2025-03-18 22:49:59.829354'), (29, 3, 2, 'COMPLETE', '2025-03-18 22:49:59.841680', '2025-03-19 01:32:58.236948'), (30, 4, 2, 'COMPLETE', '2025-03-19 01:32:58.249091', '2025-03-19 02:44:00.199931'), (31, 5, 2, 'COMPLETE', '2025-03-19 02:44:00.212032', '2025-03-19 02:48:35.289003'), (32, 6, 2, 'COMPLETE', '2025-03-19 02:48:35.301071', '2025-03-19 02:53:02.934757'), (33, 7, 2, 'COMPLETE', '2025-03-19 02:53:02.946663', '2025-03-19 05:14:51.945895'), (34, 8, 2, 'COMPLETE', '2025-03-19 05:14:51.958070', '2025-03-19 07:39:57.941087'), (35, 9, 2, 'COMPLETE', '2025-03-19 07:39:57.953283', '2025-03-19 07:44:45.050879'), (36, 10, 2, 'COMPLETE', '2025-03-19 07:44:45.063013', '2025-03-19 07:48:49.041269'), (37, 11, 2, 'COMPLETE', '2025-03-19 07:48:49.053424', '2025-03-19 07:58:10.231711'), (38, 12, 2, 'COMPLETE', '2025-03-19 07:58:10.244160', '2025-03-19 08:35:22.511800'), (39, 13, 2, 'COMPLETE', '2025-03-19 08:35:22.523859', '2025-03-19 08:37:47.411646'), (40, 14, 2, 'COMPLETE', '2025-03-19 08:37:47.423853', '2025-03-19 08:40:11.324113'), (41, 15, 2, 'COMPLETE', '2025-03-19 08:40:11.336326', '2025-03-19 08:42:56.755885'), (42, 16, 2, 'COMPLETE', '2025-03-19 08:42:56.768509', '2025-03-19 08:45:21.762313'), (43, 17, 2, 'COMPLETE', '2025-03-19 08:45:21.774375', '2025-03-19 08:50:04.836272'), (44, 18, 2, 'COMPLETE', '2025-03-19 08:50:04.848398', '2025-03-19 08:59:46.592231'), (45, 19, 2, 'COMPLETE', '2025-03-19 08:59:46.604299', '2025-03-19 09:21:23.947861'), (46, 20, 2, 'COMPLETE', '2025-03-19 09:21:23.959950', '2025-03-19 09:25:00.246089'), (47, 21, 2, 'COMPLETE', '2025-03-19 09:25:00.258355', '2025-03-19 09:27:24.047273'), (48, 22, 2, 'COMPLETE', '2025-03-19 09:27:24.059416', '2025-03-19 09:29:50.424577'), (49, 23, 2, 'COMPLETE', '2025-03-19 09:29:50.437007', '2025-03-19 09:32:16.591118'), (50, 24, 2, 'COMPLETE', '2025-03-19 09:32:16.603554', '2025-03-19 09:37:02.325258'), (51, 0, 3, 'COMPLETE', '2025-03-19 09:42:28.559598', '2025-03-19 10:00:59.060605'), (52, 1, 3, 'COMPLETE', '2025-03-19 10:00:59.074656', '2025-03-19 10:38:58.302798'), (53, 2, 3, 'COMPLETE', '2025-03-19 10:38:58.315191', '2025-03-19 10:43:00.172577'), (54, 3, 3, 'COMPLETE', '2025-03-19 10:43:00.184832', '2025-03-19 10:52:15.558593'), (55, 4, 3, 'COMPLETE', '2025-03-19 10:52:15.571022', '2025-03-19 10:55:46.646426'), (56, 5, 3, 'COMPLETE', '2025-03-19 10:55:46.658641', '2025-03-19 13:27:18.368728'), (57, 6, 3, 'COMPLETE', '2025-03-19 13:27:18.381054', '2025-03-19 13:37:55.738980'), (58, 7, 3, 'COMPLETE', '2025-03-19 13:37:55.751078', '2025-03-19 13:40:22.739622'), (59, 8, 3, 'COMPLETE', '2025-03-19 13:40:22.751977', '2025-03-19 14:22:44.460713'), (60, 9, 3, 'COMPLETE', '2025-03-19 14:22:44.472938', '2025-03-19 14:26:18.398191'), (61, 10, 3, 'COMPLETE', '2025-03-19 14:26:18.410565', '2025-03-19 14:44:47.464329'), (62, 11, 3, 'COMPLETE', '2025-03-19 14:44:47.476553', '2025-03-19 15:58:46.110336'), (63, 12, 3, 'COMPLETE', '2025-03-19 15:58:46.122697', '2025-03-19 16:03:28.546319'), (64, 13, 3, 'COMPLETE', '2025-03-19 16:03:28.558125', '2025-03-19 16:22:02.686788'), (65, 14, 3, 'COMPLETE', '2025-03-19 16:22:02.698763', '2025-03-19 16:40:40.914339'), (66, 15, 3, 'COMPLETE', '2025-03-19 16:40:40.926421', '2025-03-19 16:44:12.423735'), (67, 16, 3, 'COMPLETE', '2025-03-19 16:44:12.435699', '2025-03-19 16:46:32.256144'), (68, 17, 3, 'COMPLETE', '2025-03-19 16:46:32.268349', '2025-03-19 18:00:48.178402'), (69, 18, 3, 'COMPLETE', '2025-03-19 18:00:48.190606', '2025-03-19 20:50:27.582692'), (70, 19, 3, 'COMPLETE', '2025-03-19 20:50:27.594963', '2025-03-19 20:55:18.995823'), (71, 20, 3, 'COMPLETE', '2025-03-19 20:55:19.008108', '2025-03-19 21:13:59.682426'), (72, 21, 3, 'COMPLETE', '2025-03-19 21:13:59.694711', '2025-03-19 21:32:40.555220'), (73, 22, 3, 'COMPLETE', '2025-03-19 21:32:40.567448', '2025-03-19 21:51:24.554361'), (74, 23, 3, 'COMPLETE', '2025-03-19 21:51:24.567078', '2025-03-19 21:54:55.792592'), (75, 24, 3, 'COMPLETE', '2025-03-19 21:54:55.805852', '2025-03-19 22:13:38.340724')]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"optuna_study.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM trials\")\n",
    "print(cursor.fetchall())\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./modelNER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, ner_labels):\n",
    "  all_predictions = []\n",
    "  all_labels = []\n",
    "  for batch in dataset:\n",
    "      logits = model.predict(batch)[\"logits\"]\n",
    "      labels = batch[\"labels\"]\n",
    "      predictions = np.argmax(logits, axis=-1)\n",
    "      for prediction, label in zip(predictions, labels):\n",
    "          for predicted_idx, label_idx in zip(prediction, label):\n",
    "              if label_idx == -100:\n",
    "                #   print(label)\n",
    "                  continue\n",
    "              all_predictions.append(ner_labels[predicted_idx])\n",
    "              all_labels.append(ner_labels[label_idx])\n",
    "              #print('\\npredicted=',ner_labels[predicted_idx], '\\nlabel=',ner_labels[label_idx])\n",
    "  #print(\"all_predictions=\",[all_predictions],'\\nall_labels=',[all_labels])\n",
    "  return metric.compute(predictions=[all_predictions], references=[all_labels])\n",
    "\n",
    "#results = evaluate(model, tf_eval_dataset, ner_labels=list(model.config.id2label.values()))\n",
    "results = evaluate(model, test_set, ner_labels=list(model.config.id2label.values()))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
